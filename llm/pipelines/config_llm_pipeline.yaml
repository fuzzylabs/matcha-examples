settings:
  docker:
    dockerignore: .dockerignore
    required_integrations:
      - "huggingface"
      - "pytorch"
    requirements:
      - accelerate
steps:
  download_dataset:
    enable_cache: False
    parameters:
      # Path to directory where dataset will be downloaded
      data_dir: "data/"
  convert_to_hg_dataset:
    enable_cache: False
  get_huggingface_model:
    enable_cache: False
    parameters:
      # Name of LLM model to finetune.
      model_name: "google/flan-t5-small"
  preprocess_dataset:
    enable_cache: False
    parameters:
      # Name of LLM model to finetune.
      model_name: "google/flan-t5-small"
      # Prefix to be added to the input (required for T5 LLM family)
      prefix: "summarize: "
      # Max length of the input text
      input_max_length: 4096
      # Max length of the target summary
      target_max_length: 512
      # Split ratio for train/test
      test_size: 0.2
  tune_model:
    enable_cache: False
    parameters:
      # Learning rate
      learning_rate: 2e-5
      # Weight decay
      weight_decay: 0.01
      # Use CUDA for training
      use_cuda: False
      # Batch size per device for training
      per_device_train_batch_size: 2
      # Batch size per device for evaluation
      per_device_eval_batch_size: 2
      # Number of epochs to run tuning for
      epochs: 5
      # Load the best checkpoint at the end flag
      load_best_model_at_end: True
